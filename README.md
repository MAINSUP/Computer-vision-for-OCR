Despite a small size of training dataset, there are techniques available to force model to learn input data by introducing kernel regualizers and utilizing dropout layers along with typical convolutional and mapooling layers.
As can be seen from picture below, training loss was not oscilating much over a number of epochs. In contrast, validation loss had tendency to overfit in the middle of training process.
However, thaks to counter-overfitting methods, the final loss was steady low and accuracy was close to 1.
<img src="Training graph"> </img>
