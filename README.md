Despite a small size of the training dataset, there are techniques available to force model to learn input data by introducing kernel regularizes and utilizing dropout layers along with a typical convolutional and maxpooling layers.
As can be seen from picture below, training loss was not oscillating much over a number of epochs. In contrast, validation loss had tendency to overfit in the middle of training process.
However, thanks to counter-overfitting methods, the final loss was steady low and accuracy was close to 1.

<img src="Training graph.png"> </img>
